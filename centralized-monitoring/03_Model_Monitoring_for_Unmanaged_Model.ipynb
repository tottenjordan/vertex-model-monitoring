{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Monitoring for Unmanaged Model\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/drive/1bRsBBhlNLZYrIe59UqtdTLd91FCtXmn4\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://storage.googleapis.com/cmm-public-data/notebooks/Model_Monitoring_for_Unmanaged_Model.ipynb\">\n",
        "       <img src=\"https://www.gstatic.com/cloud/images/navigation/vertex-ai.svg\" alt=\"Vertex AI logo\">Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "ndfGn8c4bI_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Authentication"
      ],
      "metadata": {
        "id": "QNiB5LfBaft-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below.\n",
        "\n",
        "**1. Vertex AI Workbench**\n",
        "* Do nothing as you are already authenticated."
      ],
      "metadata": {
        "id": "VB_ARmwRky9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Colab, run:**"
      ],
      "metadata": {
        "id": "tz1Odi-ZkLRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "DZvan2XPagGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**3. Local JupyterLab instance, uncomment and run:**"
      ],
      "metadata": {
        "id": "6_OkGn2mlB5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! gcloud auth login"
      ],
      "metadata": {
        "id": "-Xf5yAablC_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Service account or other**\n",
        "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
      ],
      "metadata": {
        "id": "TvY0RukOlJoi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3848df1e5b0"
      },
      "source": [
        "## Step 2: Installations & Setup\n",
        "### Install the following packages to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install --upgrade --quiet \\\n",
        "    google-cloud-aiplatform \\\n",
        "    google-cloud-bigquery \\\n",
        "    pandas-gbq \\\n",
        "    'tensorflow_data_validation[visualization]<2'\n",
        "\n",
        "# Model Monitoring Experimental SDK\n",
        "! gsutil cp gs://cmm-public-data/sdk/google_cloud_aiplatform-1.36.dev20231025+centralized.model.monitoring-py2.py3-none-any.whl .\n",
        "! pip install --quiet google_cloud_aiplatform-1.36.dev20231025+centralized.model.monitoring-py2.py3-none-any.whl"
      ],
      "metadata": {
        "id": "3gT2fULbe7cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Restart the kernel (only for Colab)"
      ],
      "metadata": {
        "id": "U4o9GjHNfHdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "JaY8Q_T3fA38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup GCP Project ID and Initialize Vertex AI SDK for Python"
      ],
      "metadata": {
        "id": "g_ByJf38kdCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"[your-project-id]\" # @param {type:\"string\"}\n",
        "# set the project id\n",
        "! gcloud config set project $PROJECT_ID\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "\n",
        "REGION = \"us-central1\"\n",
        "! gcloud config set ai/region $REGION"
      ],
      "metadata": {
        "id": "5-2bPPRXkjyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Vertex AI SDK for Python\n",
        "import google.cloud.aiplatform as aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)"
      ],
      "metadata": {
        "id": "6fQqte0VAYo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store artifacts such as datasets."
      ],
      "metadata": {
        "id": "l_BdNCfk7Hjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Cloud Storage bucket\n",
        "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "3_CZRJ5L60Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ],
      "metadata": {
        "id": "lvHsyHsi7Dg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
      ],
      "metadata": {
        "id": "mrHlJi-e7AyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Prepare your production data in BigQuery"
      ],
      "metadata": {
        "id": "YCYPgHuNmSR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For unmanaged model, we currently only support consuming production prediction data from BiqQuery.**\n",
        "\n",
        "We require features in separate columns, example BigQuery schema:\n",
        "\n",
        "<img src=\"https://services.google.com/fh/files/misc/example_bq_schema.png\" width=\"400\" height=\"300\"/>\n",
        "\n",
        "Note: If you want to setup continous monitoring with a time window, a timestamp column is required.\n",
        "\n",
        "**For running this tutorial, let's create some fake serving data**"
      ],
      "metadata": {
        "id": "79ApxurGmbDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "# Define the number of rows\n",
        "num_random = 100000\n",
        "\n",
        "data = {\n",
        "    'island': [random.choice([0, 1, 2]) for _ in range(num_random)],\n",
        "    'culmen_length_mm': [random.normalvariate(50, 3) for _ in range(num_random)],\n",
        "    'culmen_depth_mm': [random.normalvariate(20, 3) for _ in range(num_random)],\n",
        "    'flipper_length_mm': [random.randint(160, 250) for _ in range(num_random)],\n",
        "    'body_mass_g': [random.randint(3000, 8000) for _ in range(num_random)],\n",
        "    'sex': [random.choice([0, 1]) for _ in range(num_random)]\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the generated data\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define the time range (start and end dates) in UTC\n",
        "# now-24h ~ now + 24h\n",
        "start_date = pd.Timestamp.utcnow() - pd.Timedelta(days=1)\n",
        "end_date = pd.Timestamp.utcnow() + pd.Timedelta(days=1)\n",
        "\n",
        "# Generate a list to store the random timestamps\n",
        "random_timestamps = []\n",
        "\n",
        "# Generate random timestamps and add them to the list\n",
        "for _ in range(num_random):\n",
        "    random_seconds = np.random.randint((end_date - start_date).total_seconds())\n",
        "    random_timestamp = start_date + pd.Timedelta(seconds=random_seconds)\n",
        "    # Format the timestamp as a string with microseconds\n",
        "    formatted_timestamp = random_timestamp.strftime('%Y-%m-%d %H:%M:%S.%f UTC')\n",
        "    random_timestamps.append(formatted_timestamp)\n",
        "\n",
        "df['timestamp'] = random_timestamps\n",
        "\n",
        "df.to_csv('production.csv', index=False)"
      ],
      "metadata": {
        "id": "im_yM1oc8l15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a BigQuery dataset and load the fake data to a table."
      ],
      "metadata": {
        "id": "kFcAMoi0-wz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "TIMESTAMP = pd.Timestamp.utcnow().strftime('%Y%m%d%H%M%S')\n",
        "\n",
        "FAKE_DATA_BQ_DATASET=f\"penguins_production_{TIMESTAMP}\"\n",
        "!bq mk --dataset $PROJECT_ID:$FAKE_DATA_BQ_DATASET"
      ],
      "metadata": {
        "id": "cWJPuv0b-g2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FAKE_DATA_BQ_TABLE=f\"{FAKE_DATA_BQ_DATASET}.data\"\n",
        "!bq load --autodetect --source_format=CSV $FAKE_DATA_BQ_TABLE \"production.csv\""
      ],
      "metadata": {
        "id": "LAmra_qq9g7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the serving logging table."
      ],
      "metadata": {
        "id": "rRHuYKS-_m-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "query_string = f\"SELECT * FROM `{FAKE_DATA_BQ_TABLE}` ORDER BY timestamp DESC LIMIT 10\"\n",
        "pd.read_gbq(query_string, project_id=PROJECT_ID)"
      ],
      "metadata": {
        "id": "2YcK0vQYbyyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Create a Model Monitor"
      ],
      "metadata": {
        "id": "RuwAoHTilv1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud.aiplatform.private_preview.centralized_model_monitoring import model_monitor\n",
        "\n",
        "my_model_monitor = model_monitor.ModelMonitor.create(\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        "    model_name=\"penguins\")\n",
        "MODEL_MONITOR_RESOURCE_NAME = my_model_monitor.name\n",
        "print(f\"MODEL MONITOR {MODEL_MONITOR_RESOURCE_NAME} created.\")"
      ],
      "metadata": {
        "id": "l_d917Ory595"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Create an on-demand Model Monitoring Job"
      ],
      "metadata": {
        "id": "uPsDU3srlzW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy files to your projects gs bucket to avoid permission issues.\n",
        "# Ignore any error(s) for bucket already exists.\n",
        "PUBLIC_TRAINING_DATASET = \"gs://cmm-public-data/datasets/penguins/training_100k.csv\"\n",
        "TRAINING_DATASET = f\"{BUCKET_URI}/penguins/training_100k.csv\"\n",
        "\n",
        "! gsutil copy $PUBLIC_TRAINING_DATASET $TRAINING_DATASET"
      ],
      "metadata": {
        "id": "BEFgQe8zjXVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "EMAIL=\"[your-email-address]\" # @param {type:\"string\"}\n",
        "\n",
        "# Skew and drift thresholds.\n",
        "DEFAULT_THRESHOLD_VALUE = 0.001\n",
        "\n",
        "SKEW_THRESHOLDS = {\n",
        "    \"culmen_length_mm\": DEFAULT_THRESHOLD_VALUE,\n",
        "    \"body_mass_g\": 0.002,\n",
        "}\n",
        "\n",
        "# Prediction target column name in training dataset.\n",
        "GROUND_TRUTH = \"species\"\n",
        "\n",
        "TIMESTAMP = pd.Timestamp.utcnow().strftime('%Y%m%d%H%M%S')\n",
        "JOB_DISPLAY_NAME = f\"churn_model_monitoring_job_{TIMESTAMP}\""
      ],
      "metadata": {
        "id": "JMjhTEG2AHzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start a monitoring job for the skew detection(training vs serving).\n",
        "In this example, training data is a csv file from Google Cloud Storage and the serving data is from BigQury. We support two options for connection:\n",
        "\n",
        "* table_uri: This is what the example shows, it will consume all the features from the table.\n",
        "* query: Actually it's SQL query, you could select the features you are interested for analysis, be sure to include the timestamp column if you'd like to specify the data window or want the continous monitoring."
      ],
      "metadata": {
        "id": "WGScVVQ4EI6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_monitoring_job=my_model_monitor.run(\n",
        "    display_name=JOB_DISPLAY_NAME,\n",
        "    objective_config=model_monitor.spec.ObjectiveSpec(\n",
        "        baseline=model_monitor.spec.MonitoringInput(\n",
        "            gcs_uri=TRAINING_DATASET,\n",
        "            data_format=\"csv\",\n",
        "            ground_truth_field=GROUND_TRUTH),\n",
        "        target=model_monitor.spec.MonitoringInput(\n",
        "            table_uri=f\"bq://{PROJECT_ID}.{FAKE_DATA_BQ_TABLE}\",\n",
        "            timestamp_field=\"timestamp\"),\n",
        "        feature_distribution_skew=model_monitor.spec.SkewSpec(\n",
        "            default_threshold=DEFAULT_THRESHOLD_VALUE,\n",
        "            feature_thresholds=SKEW_THRESHOLDS,\n",
        "            # The data window of the serving data is \"2h\", indicating the selection of '2-hour' data windows before the current time for analysis.\n",
        "            window=\"2h\")\n",
        "    ),\n",
        "    notification_config=model_monitor.spec.NotificationSpec(\n",
        "        user_emails=[EMAIL],\n",
        "    ),\n",
        "    output_config=model_monitor.spec.OutputSpec(\n",
        "        gcs_base_dir=BUCKET_URI\n",
        "    )\n",
        ")\n",
        "\n",
        "CMM_JOB_RESOURCE_NAME = model_monitoring_job.name\n",
        "print(f\"Model Monitoring Job {CMM_JOB_RESOURCE_NAME} created.\")"
      ],
      "metadata": {
        "id": "S-3ilL5-03k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Wait for the Model Monitoring Job to finish and verify the result"
      ],
      "metadata": {
        "id": "nL10YABxl47v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check email"
      ],
      "metadata": {
        "id": "LrSU6d0xFfzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Here's a sample create job email...\n",
        "\n",
        "<img src=\"https://services.google.com/fh/files/misc/ad_hoc_email.png\" />"
      ],
      "metadata": {
        "id": "zw5KodgebDVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### If there is any anomaly detected, you will receive an email like\n",
        "\n",
        "<img src=\"https://services.google.com/fh/files/misc/ad_hoc_anomalies_email.png\" />"
      ],
      "metadata": {
        "id": "37SPm_majLVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check GCP Console"
      ],
      "metadata": {
        "id": "LPC6ZoerifM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check the \"Monitor\" tab under \"Vertex AI\"\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/cmm-public-data/images/unmanaged_job_console.gif\" />"
      ],
      "metadata": {
        "id": "4Pw6Z-bVbZaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify Output GCS bucket after job is finished"
      ],
      "metadata": {
        "id": "MFZcBnwsdSfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_model_monitor.show_skew_stats(model_monitoring_job_name=CMM_JOB_RESOURCE_NAME)"
      ],
      "metadata": {
        "id": "NhY20BTjjkVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Schedule Continous Model Monitoring\n",
        "\n",
        "If you are interested at trying continous model monitoring, please following the example below to create a schedule. You could create multiple schedules for your monitor.\n",
        "\n",
        "This example is to run the model monitoring job every 1 hour at 00:00, 01:00 ... Everytime \"1h\" window data will be analyzed. Let's say a job is scheduled to run at 6:00am, then 5:00 am ~ 6:00 am data will be consumed for analysis."
      ],
      "metadata": {
        "id": "Uj2bev2rcSsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CRON=\"0 * * * *\" # @param {type:\"string\"} Every 1 hour at :00, for example 1:00, 2:00..\n",
        "SCHEDULE_DISPLAY_NAME=\"penguins-continous-skew-detection\"\n",
        "DATA_WINDOW=\"1h\""
      ],
      "metadata": {
        "id": "FLf9rUl7cs8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_monitoring_schedule=my_model_monitor.create_schedule(\n",
        "    display_name=SCHEDULE_DISPLAY_NAME,\n",
        "    cron=CRON,\n",
        "    objective_config=model_monitor.spec.ObjectiveSpec(\n",
        "        baseline=model_monitor.spec.MonitoringInput(\n",
        "            gcs_uri=TRAINING_DATASET,\n",
        "            data_format=\"csv\",\n",
        "            ground_truth_field=GROUND_TRUTH),\n",
        "        target=model_monitor.spec.MonitoringInput(\n",
        "            table_uri=f\"bq://{PROJECT_ID}.{FAKE_DATA_BQ_TABLE}\",\n",
        "            timestamp_field=\"timestamp\"),\n",
        "        feature_distribution_skew=model_monitor.spec.SkewSpec(\n",
        "            default_threshold=DEFAULT_THRESHOLD_VALUE,\n",
        "            feature_thresholds=SKEW_THRESHOLDS,\n",
        "            window=DATA_WINDOW),\n",
        "\n",
        "    ),\n",
        "    notification_config=model_monitor.spec.NotificationSpec(\n",
        "        user_emails=[EMAIL],\n",
        "    ),\n",
        "    output_config=model_monitor.spec.OutputSpec(\n",
        "        gcs_base_dir=BUCKET_URI\n",
        "    )\n",
        ")\n",
        "\n",
        "SCHEDULE_RESOURCE_NAME = model_monitoring_schedule.name\n",
        "print(f\"Schedule {SCHEDULE_RESOURCE_NAME} created.\")"
      ],
      "metadata": {
        "id": "jb4GcI6Zcvof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You could check out your schedules in Console\n",
        "<img src=\"https://storage.googleapis.com/cmm-public-data/images/unmanaged_continous.gif\" />"
      ],
      "metadata": {
        "id": "eVp3k1XQezvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Clean Up (after job finished)"
      ],
      "metadata": {
        "id": "hovSbsmBmEKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# When no jobs are running, delete the schedule and all the jobs.\n",
        "my_model_monitor.delete_schedule(SCHEDULE_RESOURCE_NAME)\n",
        "my_model_monitor.delete_all_model_monitoring_jobs()\n",
        "my_model_monitor.delete()\n",
        "\n",
        "# Delete BQ logging table\n",
        "bqclient = bigquery.Client(project=PROJECT_ID)\n",
        "# Delete the dataset (including all tables)\n",
        "bqclient.delete_dataset(FAKE_DATA_BQ_DATASET, delete_contents=True, not_found_ok=True)"
      ],
      "metadata": {
        "id": "ejHg5PqiJPS2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}